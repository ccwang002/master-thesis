\chapter{Methods}
\label{c:method}

In the following sections, first a number of typical RNA-Seq and DNA-Seq
pipelines that BioCloud supports are defined and explained. Then the design of
BioCloud is explained by breaking down into two main parts: the website and the
report generator. For the website, the components including account
registration, data sources management, experiment design, and pipeline
execution are described and their roles are illustrated in the website overview
workflow. For the report generator, the workflow is shown to demonstrate how
outputs of each tool is extracted, parsed, and rendered into pre-defined
web-based report templates.



\section{RNA-Seq pipelines}
\label{s:rnaseq-pipeline}

The RNA-Seq analysis pipelines that BioCloud supports are summarized in
Figure~\ref{fig:rnaseq-pipeline}, where some of the tools are shared for
different pipelines. The goal of RNA-Seq analysis is to infer differential
expression of gene or transcripts across different conditions. Based on
different assumptions of the distribution of gene and transcript expression,
three different methods are used: Cufflinks, DESeq2, and kallisto, which yield
FPKM based, count-based, and abundance-based expression value respectively.

\input{figures/rnaseq_pipelines}

Cufflinks \cite{trapnell2010:transcript} and its differential analysis method
Cuffdiff \cite{trapnell2013:differential} are best used for gene expression
considering its isoforms, e.g., differential expression on transcript-level.
The output of Cuffdiff can be further visualized using R package cummeRbund
\cite{:cummerbund} from the same development team of Cufflinks. The unit of
expression Cufflinks uses is reads per kilobase of exon per million reads
mapped (FPKM). For DESeq2 \cite{love2014:moderated}, its model analyzes
expression change on gene level using read counts, which cannot be applied to
transcript level analysis. The unit of expression DESeq2 uses is normalized
read counts. The read counts are computed by featureCounts
\cite{liao2014:featurecounts}. Another popular alternative for computing read
counts is HTSeq \cite{anders2015:htseqa}. HTSeq generally acts the same as
featureCoutns while having slower processing speed so it is not used in
BioCloud.

Both Cufflinks and DESeq2 require genomic location of reads so their analysis
is prepended by the genome alignment step. Three aligners are supported:
Tophat2 \cite{kim2013:tophat2}, HISAT2 \cite{kim2015:hisat}, and STAR
\cite{dobin2013:star}. Though the underlying alignment algorithms of these
tools are different, all of their alignment output in BAM file format can be
used interchangeably by the following expression inference tools. The most
discernible difference is computation time. Given the same execution
environment on a same sequencing sample, Tophat2 takes a hundred times the
computation time of STAR to complete the alignment. HISAT2 uses even slighter
shorter time and significantly less memory than STAR.

Between the genome alignment and expression inference step, an optional step of
removing PCR duplicated reads can be added, which can be done by Picard
\cite{:picard} or Samtools \cite{li2009:sequence}. PCR duplication can occur in
some NGS technology that requires PCR amplification when the given biological
sample has low cDNA template diversity. Filtering out PCR duplication can rule
out the expression bias made by PCR and reduce the computation time due to
fewer reads. However, PCR duplication removal can introduce extra artifact
since no information about PCR duplication can be obtained from the sequencing
data.  That is, the data cannot tell whether a sequence read is PCR duplicated
or not from the sequencing result. Currently the tools label a sequence read as
PCR duplicated if its 5' and 3' genomic position is exactly identical to
another read. Since this empirical rule clamps the dynamic range of gene and
transcript expression, user can choose to skip the step if their NGS technology
does not involve PCR or the PCR duplication effect is negligible.

Contrary to aligning RNA-Seq reads to genome, the concept of pseudoalignment
which does not require actual genome alignment has been brought up in the last
two years. For each sequence read, pseudoalignment finds the most compatible
transcript by sharing the most number of k-mers and use the abundance of k-mers
to infer the estimated the expression of the transcript. By skipping the step
of spliced genome alignment, transcript quantification can be fast and maintain
roughly as accurate as the actual alignment. The idea of pseudoalignment is
first proposed by Salifish \cite{patro2014:sailfish}, formalized and continued
by kallisto \cite{bray2016:nearoptimal} and Salmon \cite{patro2015:accurate}.
Though the differential expression analysis can utilize the existed count-based
analysis tools by rounding the estimated abundance of all transcripts and
summing them to gene level expression, their statistical models on read count
distribution are different. Thus additional tools are developed to provide
better integration with pesudoalignment results, including sleuth
\cite{pimentel2016:differential} and SUPPA \cite{alamancos2015:leveraging}.
Since pseudoalignment is a relatively new concept, only one pipeline is
implemented on BioCloud. User can use kaillisto and sleuth, tools developed by
the same lab, to do pseudoalignment differential expression analysis.

Raw sequence reads are first quality checked using FastQC \cite{:fastqc}.
FastQC provide comprehensive visualization of the quality of sequencing
experiment including per base quality plot, sequencing duplication rate plot,
and adapter sequencing check. FastQC does not affect the content of sequences
thus this step is optional.



\section{DNA-Seq pipelines}
\label{s:dnaseq-pipeline}

The DNA-Seq analysis pipelines that BioCloud supports are summarized in
Figure~\ref{fig:dnaseq-pipeline}. Some of the tools overlap with RNA-Seq
pipelines which have been already mentioned and described in
Section~\ref{s:rnaseq-pipeline}. To find germline variants, sequence reads are
first aligned to genome by the tool Burrows-Wheeler Alignment (BWA)
\cite{li2009:fast} using its MEM local alignment algorithm.

\input{figures/dnaseq_pipelines}

For variant calling, BioCloud supports two callers: VarScan
\cite{koboldt2012:varscan} and GATK
\cite{vanderauwera2013:fastq,mckenna2010:genome}. To use with VarScan, the
output alignment BAM file is processed by samtools' mpileup subcommand to
compare the aligned reads with genome reference sequence. VarScan take the
mpileup output and determine if it is a significant variant that beyond the
given threshold for each base. To use with GATK, the best practice
\cite{vanderauwera2013:fastq} made by \citeauthor{vanderauwera2013:fastq} is
adopted. However, since our lab does not use GATK for variant calling as
frequently as VarScan, the development to support GATK has been postponed.
Currently existed GATK protocols in our lab are for GATK 1.x, which are
outdated and are not compatible with the newer GATK 2.x.

After variant calling, these variants are further annotated with gene
information, SNP database records, animo acid and protein structure change
predictions by ANNOVAR \cite{wang2010:annovar}. ANNOVAR collects widely used
genome references such as refSeq and Ensembl, annotation databases such as 1000
Genome, dbSNP, ClinVar and COSMIC, and prediction algorithms such as SIFT and
PolyPhen-2. By relying on ANNOVAR, BioCloud don't need to look up each database
and reference on its own, which also reduces the maintaining burden for keeping
all database in use updated.



\section{BioCloud Website}
\label{s:biocloud-website}

In this section, the architecture of BioCloud website is introduced and an
overall picture is given. The workflow for generating summary report is
explained in Section~\ref{s:report-generation}. Except for summary report
generation, the mechanism of each component is explained per subsection,
including a message authentication method for access control and account
validation; experiment design for grouping samples in conditions and is reused
for multiple analyses; analysis submission with parameter settings; job queue;
and finally access control for report and results of an analysis.



\subsection{Overview}

Shown in Figure~\ref{fig:overview-arch}, BioCloud website can be viewed as
three main parts: data storage, computing cluster, and web frontend. For normal
user, they only interact with the web frontend and the architecture of BioCloud
will remain unknown for them. However, in the BioCloud internal structures,
these main parts can exist on different machines. So when large computing
resource is needed, more machines can be added to the computer cluster without
modifying the settings and source code of BioCloud. Data storage includes the
database and local file system to store data sources, genome references and
analysis results and reports. File systems like NFS can be used to allow access
from multiple machines if BioCloud operates across one and more servers.

\input{figures/overview_arch}

We further breakdown this general BioCloud architecture into independently
operating components and demonstrate how these components interact in a
workflow of a user completing one analysis in
Figure~\ref{fig:overview-workflow}. Before going down the workflow explanation,
some terminologies BioCloud use should be clarified at first. A data source
represent an file on file system. A sample can be a collection of multiple data
sources. For example, a pair-end sequencing sample contains two data sources
corresponding to the R1 (or forward strand) and the R2 (or reversed strand) of
the sequencing result.

\input{figures/overview_workflow}

The journey of the analysis starts by adding new sequencing samples. These new
samples are added under the user's data source directory. During the auto
discovery, sample name, file type and other information of a source are guessed
by BioCloud but allowed for user to override them. An additional checksum can
be supplied for each source to confirm the data integrity. Data sources or
samples come in group of biological experiments. For example, a experiment can
have control group and test group with 4 samples each. In BioCloud we define
experiment as well, which is a collection of samples. In an experiment, each
selected sample is assigned a condition which can be used for grouping later-on
stages such as differential expression analysis. An experiment can be used in
one and more analyses such as trying different analysis parameters or using
different analysis pipeline. Therefore, an experiment contains which samples to
use and how they are grouped in terms of sample conditions.

Finally, user can now submit an analysis by specifying a pre-defined experiment
and tuning the parameters available for the chosen analysis. User is also
required to select the genome reference used for the analysis, whose possible
options include hg19 (UCSC), hg19 (Ensembl), and hg38 (NCBI). After configuring
and submitting the analysis, the analysis creates its corresponding job and
wait in the job queue for computing cluster to execute. Each analysis pipeline
defines its own unique sequence of execution stages. During the job execution,
the status of these stage reflect either a stage is successfully completed,
failed, running, skipped by user configuration, or waiting for the previous
stage to end. User can obtain the stage status through by monitoring jobs and
the queue. The summary report generation is automatically appended at the end
of each analysis, which is explained in Section~\ref{s:report-generation}. Once
the analysis completes, user will be notified by email with link to the
information page of the analysis.

User can view the summary report by the given link online. Also, outputs of all
executed tools are organized under the folder allocated for this analysis. User
can find the link to download these outputs as well. The report can be shared
to others without a BioCloud account by simply sharing the link. The link to
results and report can be set as private access or have a new URL if user does
not want to disclose their report and result anymore. By pasting the link to
results on genome browser like Ensembl and UCSC, user can immediately view
their results with other genome annotations without uploading full output file
to the genome browser.

\input{figures/biocloud_erd}

So far all major components of BioCloud have been covered and they will be
explained in detail in the following subsections. The corresponding database
design is shown in Figure~\ref{fig:biocloud-erd}.


\subsection{Data integrity check and authentication}

Data integrity check and authentication are important topics in BioCloud
website. Data sources need to be checked if the content of uploaded file is
identical to that of user's local copy. During account activation, an unique
activation link for this account is generated and sent to the email registered
by the account. When the user clicks the link, BioCloud knows which account is
activated and is certain that user actually posses this email address. For
sharing report or result through given link, BioCloud verify the link is valid
and retrieve the corresponding analysis report or result. Except for data
integrity check, we have to make sure all the links mentioned above cannot be
forged in a reasonable time given the current computing power. Therefore, in
BioCloud several methods are used to make sure the data integrity and to
validate authentication link and code.

To check data integrity of upload data sources, a checksum per file using
SHA2-256 hash algorithm can be passed during the data source discovery can be
provided. Checksum summarizes the file into a fixed length of bits no matter of
the file size. The checksum algorithm is sensitive to minor changes, so if a
file have a few bytes corrupted during the network transmission, its checksum
will be drastically different to the correct counterpart. Checksum is not file
compression so it can only be used for data integrity check. The checksum of
two different files can be identical, the incident being referred to as
collision. The checksum algorithm in use, SHA2-256, generates 256bits long
checksum.

For authentication, hash-based message authentication code (HMAC) is used to
create the authentication key. A HMAC string consists of two parts: the
original message and a signature for the original message, which is generated
with a secret key and a salt string in the current BioCloud implementation. The
signature is a hash is generated based on the given secret key concatenating to
salt string and the original message. Since the secret key is not disclosed to
user, knowing the original message and the salt string cannot re-create the
signature. The only way to forge the signature is through hash collision.
However, by choosing cryptographic hash function such as SHA-1\footnote{
    SHA-1 is not considered collision resistant anymore, though it is still
    suitable to be used as the hash function of HMAC.
} and SHA-2, they are collision resistant so it is hard to find a collision
in feasible time. The salt key, public or private, extends the computation
effort to crack the underlying secret key. By passing JSON as the original
message, one can store complex data structure in one HMAC string.


\subsection{User account management}

To let individuals manage their own data sources, analyses and results,
BioCloud builds its user account management based on Django's implementation.
Security is one of the top concern for BioCloud, we rely on the password
management made by Django which is secure and long tested in the public.
BioCloud requires email and password for registration.

Since it sends out email notification to users, the email address is first
verified by sending an activation link upon registration. The link contains a
HMAC token consisting of user's email address and registration timestamp. The
timestamp can verify if the activation link has been expired. The activation
link is generated on-the-go when a user requests. Compared to alternative
solution to store activation links in the database, our approach does little
performance impact on database system. A user is marked as verified once the
account activation is complete.

To better maintain the BioCloud, special account types, staff and superuser
account, can access to the BioCloud admin interface and view and modify all
users' data and activities. Superuser doesn't need to permission to do all
operations in the admin interface, whereas staff can only performance a subset
of granted operations. The account type can be adjusted by
\texttt{is\_superuser} and \texttt{is\_staff} columns of
\texttt{users\_emailuser} table, as shown in Figure~\ref{fig:biocloud-erd}.
User's data sources and experiments, and analyses are referred back to its user
record via \texttt{owner\_id} column on their respective table.


\subsection{Data source management}

Once the account is initiated, BioCloud create a folder for user to store
data sources based on the user ID. User ID is auto generated and unique. User
can either move or link their data sources under this folder.

User then add these data sources using ``data source discovery'' function of
BioCloud. It scans user's data source folder and filter newly added sources
with the user's database records. Sample name and type of the data source is
guessed by BioCloud. After the discovery, user can override the guessed values
and add checksum for each discovered files. User can later change these
property after the data sources are added as well.

The representation of data source in the database is a straight-forward
mapping, as table \texttt{data\_sources\_datasource} shown in
Figure~\ref{fig:biocloud-erd}. However, there are some metadata that are only
required for certain file types. For example, only FASTA and FASTQ files can be
pair-end, e.g., R1 and R2. Metadata is stored as a column of JSON type.
Metadata can be a hierarchical data structure in one column and database won't
complain if some substructure of a metadata record cannot be found in another.


\subsection{Experiment design}

Experiment collects a group of data sources and labels them with conditions. An
example of experiment design is shown in Table~\ref{tab:experiment-example}. In
this experiment, four samples, A, B, C and D, from user ID 1 with pair-end
FASTQ files, are selected. The former two samples are of condition control,
while rest of the samples are of condition testing.

\input{tables/experiment_example}

BioCloud creates an interactive frontend for user to filter and select the data
sources they want to include in this experiment. Since a simple may have
multiple data sources, by default BioCloud guesses using the sample information
during discovery. However, user can override the sample name during experiment
design. For example, user can create a new sample called AC and include all
four data sources originally of sample A and C. After finalizing the samples,
user defines the available conditions for this experiment and assign sample to
them.

To sum up, there are three steps to design an experiment: select data sources,
label sample name on data sources, label condition on samples. However, user
can freely go back and modify result of the previous stage. BioCloud will try
to re-use all user given information maximally as possible so user don't need
to re-fill all the information again. For example, if the condition ``Control''
is misspelled as ``Contorl'', when updating the condition name, all samples of
the condition will remain and their condition names will be updated
simultaneously. BioCloud achieves this feature by binding condition and sample
name only on data sources' metadata, and user updates directly to the metadata.
When a metadata is changed, BioCloud decides which part of the experiment
design is changed and re-renders all the affected output. This is the notion of
web frontend development called reactive programming.

User can give experiment name and description to help manage all defined
experiments. Description is written in Markdown syntax \cite{:commonmark},
which is a lightweight markup language that convert plain texts into styled
document. Users are encouraged to put relevant links and references regarding
to the experiment in its description. Once an experiment is created, user will
no be able to modify the condition and sample design. The fixation of
experiment design ensures all analyses depending on it will have the same
setting of conditions and samples.


\subsection{Genome reference}

Genome reference store all related references and annotations for a given
version of genome reference. For human, there are hg38 (UCSC), hg38 (Ensembl
Release v84), and hg19 (UCSC); for mouse, there are mm10 (UCSC) and mm10
(Ensembl v84). All tools required index files and the annotations should be
available for every supported genome reference. For example, if one decides to
support hg38 (UCSC) genome reference, all aligners' genome index including
HISAT2, STAR, Bowtie and BWA needs to be generated; dbSNP needs to be version
141 and latter to support hg38 genome reference. It should be noted that
primary assembly of genome won't change across patch releases. So primary
assembly sequence of GRCh38 and GRCh38.p5 remain the same.

Genome reference are managed via the table
\texttt{analyses\_genomereference} using BioCloud admin interface. By providing
the release data of the genome reference, BioCloud can sort the reference in
chronological order. If a newer genome reference comes out, one can specify the
upgrade path in the column \texttt{newer\_reference\_id} so downstream analysis
tools can provide a better hint to user about the new genome reference release.
Genome references are all stored under a same folder specified through the
BioCloud's setting file. Its internal folder structure resembles that of
iGenome \cite{:igenomes}.


\subsection{Analysis submission}

A new submission analysis requires selection of analysis pipeline type and
experiment design. First user selects one of the analysis pipelines described
in Section~\ref{s:rnaseq-pipeline} and \ref{s:dnaseq-pipeline}. After an
analysis is chosen, the webpage is directed to the corresponding form page to
adjust the analysis parameters. There are some common parameters for all
analyses pipelines, including name, description, genome reference and
experiment in use. Like the description of experiment, description of analysis
is written in Markdown to provide basic markups like hyperlinks and bullet
lists. When choosing the experiment, its condition and sample design is shown
dynamically.

Parameters of an analysis pipeline include the choose of tools of equivalent
function and parameters passing to the chosen tools. As shown in the
Figure~\ref{fig:biocloud-erd}, a RNA-Seq pipeline analysis that uses Cufflinks
for differential expression analysis. All of the possible parameters are
stored as separate columns. However, only a subset of the parameters will be
used, mainly depending on the genome aligner user chooses (user's choice is stored
at \texttt{genome\_aligner} column). To avoid user confusion, the form that
renders the analysis parameters will dynamically display the corresponding
parameter set given user's choices. For example, if user chooses HISAT2 as the
genome aligner, all options for Tophat2 and STAR will be hidden. The dynamic
display rendering is also handled by reactive programming in web frontend.


\subsection{Job queue management}

% describe what happens when a new analysis is submitted
% - analysis detail created (stage)
% - report and result directory created
% - mark the analysis as queueing

As a new analysis is submitted by user, it first initiates extra settings to
describe the job execution detail, e.g., stage status, and allocate folders to
store the result and report of the analysis. A job execution detail group an
analysis pipeline into execution stages, whose status can be waiting, running,
successful, failed, or skipped. Take the example RNA-Seq analysis pipeline
shown in Figure~\ref{fig:biocloud-erd}. Its job execution detail is stored in
table \texttt{rna\_seq\_rnaseqexedetail} and defines four stages: qc,
alignment, cufflinks, and cuffdiff. If user opts out quality
check upon setting the analysis, it won't execute FastQC and the qc stage will
be marked as skipped. Stage status will be shown when user query about the
current analysis execution status. Additionally, new analysis will create a new
report record with an unique ID. This ID may differ with the analysis ID since
same analysis ID may appear in different types of analysis pipelines. Folder
for result and report of the analysis will be created based on the report ID.
Finally, the analysis execution status is marked as queueing and wait for a
work of the computer cluster to pick up.

% computer cluster job queue and worker
% - get the analysis ID
% - one machine only have one worker

Computer cluster runs its job queue and job workers in separate processes. The
queue monitor continuously watches if a new job, e.g. a new analysis, is added
to the job queue. When a new job is added, it pass down to one of its available
worker to execute. The job queue BioCloud uses guarantees at least once
delivery. The job worker receives a new job with the analysis ID and obtains
all related information by querying the database with this analysis ID. Here
the analysis ID is used for analysis of different pipelines won't share the
task they run so no ID collision issue. During the execution, work updates the
stage status in the way previously mentioned. When the analysis is completed
and the summary report is generated, the worker will notify the user about the
completion via email.


\subsection{Report and result access control}

% what is in the report HMAC token
% - email, auth number, report ID
% no login for this link set the report link as public or private change the
% report HMAC token canonical link to the report
To enable report and result sharing, HMAC token is used to control the access.
The access URL patterns for a report and the result file of the same analysis
are shown in the following, which share the same authentication key
\texttt{<auth key>}:

\begin{CVerbatim}[fontsize=\small]
https://<biocloud domain>/access/<auth key>/report/
https://<biocloud domain>/access/<auth key>/result/path/to/file
\end{CVerbatim}

\vspace{-1em}\noindent
Access to the report and result files via these URLs does not require the
viewer to be logged in on BioCloud or to be the owner of the analysis.

Authentication key contains the user's email, a customizable authentication
number, and the report ID. The user's email is used to prevent user on BioCloud
from forging access to other user's report easily. Authentication number is
decided per user which can be modified in the user profile setting. If the
number is changed, all authentication keys of this user will be updated and
invalidate the previous ones. The number is useful when user accidentally
disclose one's report or result link in public or user finishes connecting the
data with external genome browser. Finally, the report ID is stored in the
authentication key so BioCloud understands which report to retrieve. Aside from
the key, user can control to let individual report to be public or private.

When the BioCloud server receives a request for a report or a result file, it
first validates the authentication key and extract the report ID. If the key is
invalid, a HTTP 404 error of invalid access is raised. Upon any check below
fails, the same HTTP 404 error is raised. If the key is valid, it further
checks if the current authentication number of user match the one extracted
from the key. Then, it checks if the requested report is public or private. For
private report, viewer must be already logged in and must be the owner of the
report. After all above checks are passed, BioCloud serves the report content
or the result file specified by the rest of the URL.

Since the report and result link can be changed frequently, BioCloud also
provide a canonical access link for report, which redirects user to the
currently valid report URL, shown in the following:

\begin{CVerbatim}[fontsize=\small]
https://<biocloud domain>/access/report/<report ID>/
\end{CVerbatim}

\vspace{-1em}\noindent
Only the logged in owner of the report can use the canonical link, so it cannot
be shared with others.



\section{Report generation}
\label{s:report-generation}

Summary report shown in Figure~\ref{fig:overview-workflow} is generated after
an analysis job finishes. A more detailed report generation workflow is shown
in Figure~\ref{fig:bcreport-workflow}. A standalone program, BCReport, that
parses a set of results of NGS analysis tools and generates the summary report.
By specifying the structure of data sources, samples and conditions, BCReport
can provide summary report for any results which need not to be generated on
BioCloud. The standalone mode of BCReport extends the ability to parse a more
general set of NGS results. Moreover, for tools that BCReport does not support,
developers can extend the currently available report templates and result
parsers to build their own analysis pipelines.

\input{figures/bcreport_workflow}


\subsection{Analysis result structure and information}

A result folder structure readable for BCReport is shown in
Figure~\ref{fig:bcreport-workflow}, where each tool allocates its own result
folder. The folder name for the result of a tool can be flexible. For example,
for folder that contains result of STAR, its name can be \texttt{STAR} or
\texttt{2\_STAR}. Numerical prefix can help file system sort the result folders
to reflect their execution order. Inside the result folder of a tool, there are
four main types of grouping structure: by data sources, by samples, by
conditions and no grouping structure at all. The grouping structure depends on
the execution type of tool. For example, FASTQ runs quality check per data
source, thus its results are grouped by data sources; STAR runs alignment per
sample; Cuffdiff runs differential expression analysis and outputs the result
per condition; Cuffmerge combines transcript annotations of all samples and
produces one output for an analysis. BCReport's tool parser understands how
each tool is executed and will parse its result based on the corresponding type
of grouping structure.

% result folder structure
% analysis inform in YAML
%   data source, sample, conditions, parameters

To let BCReport understand how the analysis was executed, analysis information
including data sources, samples, conditions, and tool parameters are stored in
an analysis information file \texttt{analysis\_info.yaml} in YAML \cite{:yaml}
format. As shown in the Figure~\ref{fig:bcreport-workflow}, there are three
sections of the analysis information: conditions, data sources, and parameters.
In conditions section, grouping of samples and conditions are specified. In
data sources section, the detail of all data sources in use is given. In
parameters section, tool parameters specified during analysis submission are
recorded. Additional analysis information will be places in this section as
well.

As long as the analysis results are organized in the folder structure stated
above and an analysis information YAML file is provides, BCReport can recognize
the analysis and provide the summary report. However, when BCReport is used
with BioCloud, it can also retrieve the raw data access links from BioCloud and
display them in the summary report.


\subsection{BCReport: result processing framework}

% HTML template rendering
When the BCReport is called, it first parses the analysis results based on the
analysis information. An additional parameter is passed to instruct BCReport
which analysis pipeline parser is used to extract the given analysis result. An
analysis pipeline parser contains the logics to extract the result and the
corresponding templates to display. Since an analysis pipeline may have
different tool execution branches, the template rendering contains conditional
blocks so only chosen tools will be processed. For example, for a analysis
based on Cufflinks RNA-Seq pipeline described in
Figure~\ref{fig:rnaseq-pipeline}, there are three possible genome aligners.
Based on the analysis result shown in Figure~\ref{fig:bcreport-workflow}, STAR
is used for genome alignment. Therefore, result sections about Tophat2 and
HISAT2 will be skipped and not shown in the summary report. However, to prevent
unnecessary complexity in the analysis pipeline design, users are not
encouraged to create too many execution branches in a pipeline. When used with
BioCloud, BioCloud set a special flag of BCReport to display the URLs to
original data files. These URLs will contain the valid access token.

All tools defined in current BioCloud analysis pipelines can be re-used by
users to design new custom analysis pipeline. Pipelines of BCReport are
designed to be inheritable and reusable. The extension on existed pipeline is
discussed in Section~\ref{s:pipeline-extension}.



\section{Implementation}

\input{figures/overview_arch_tool}

An overview of BioCloud implementation and its tool dependencies is shown in
Figure~\ref{fig:overview-arch-tool}. BioCloud implementation is described by
breaking down into website, deployment and report three main parts in the
following sections.


\subsection{Website}

% Standard website:
%   Django
%   PostgreSQL
% Frontend:
%   Django-Q
%   Vue.js
%   Bootstrap
% Computation cluster
%   Django-Q
BioCloud website runs on Python 3.5 with web framework Django 1.9. For database
system, BioCloud requires PostgreSQL 9.4 or 9.5. BioCloud code base
communicates with the database via Django Object-relational Mapping (ORM),
which continually checks with consistency between the database scheme and the
mapped object representation in source code. All data in the database can be
correctly migrated to match the representation of the new version of BioCloud
using Django ORM.

For website frontend, Boostrap 3 and jQuery 2 are used for basic HTML
rendering and user interaction. Markdown fields such as description of
experiment and analysis are implemented by Javascript-based editor, SimpleMDE.
In database these fields are recorded as-is and treated as normal text
data type. For more complicated user interaction, notion of reactive
programming is adopted and implemented based on Vue.js. In Vue, the html
elements (DOM) are reactive based on its MVVM structure, as shown in
Figure~\ref{fig:vuejs-mvvm}. DOMs are bounded to data in mutual way. When data
are updated, Vue finds the DOMs that render the affected data and render
them. Some DOMs can be defined to modify its underlying data, which are shared
and sync in the same Vue app.

\input{figures/vuejs_mvvm}

For computing cluster, the job queue is implemented by Django-Q. Compared to
the more popular alternative, celery, it can depend only on database and Django
ORM as its job queue and message broker. It can be re-configured to rely on
more efficient brokers and queues including Redis, IronMQ, and Amazon SQS.
Using database is sufficient for the current workload of BioCloud. BioCloud
does not rely on information stored in the Django-Q's job queue. However, those
information can be found at the BioCloud backend so administrator can manually
control the execution of job queue.


\subsection{Deployment}

% Ubuntu 16 OSX
% Let's Encrypt
% nginx systemd
% nginx x-Accel and Django auth integration

BioCloud public server is running at \url{https://biocloud.cgm.ntu.edu.tw/} on
operating system Ubuntu 16.04 LTS and web server nginx 1.10. BioCloud's Django
application, e.g. accepting web requests and connections, are managed by uWSGI
server and then reversely proxied to nginx. Static files are hosted directly by
nginx for efficiency. For static files that require Django authentication,
nginx's X-accel HTTP request header is added to allow the internal routing.
Thus those files will be first checked by the web application. If the viewer
has valid access permission, Django set the header and redirect the request to
nginx, who serves the actual content of the request file. Processes of uWSGI
and Django-Q are managed by systemd. BioCloud can also run in the development
mode on OSX, where all functionalities of BioCloud can be tested and executed.
Mail sending server is rallied by FastMail's SMTP server. SSL certificate that
enables HTTPS connection is verified via ACME protocol and issued by Let's
Encrypt. Each newly issued SSL certificate is valid for 90 days and the renewal
is automated by systemd.


\subsection{Report}

% Jinja2
% skbio
% Vue.js
% plotting libraries
% Bootstrap
% highcharts.js

BCReport runs on Python 3.5 and uses the Anaconda Python distribution. Jinja2
template language as the HTML template of summary report. For parsing the
analysis result, pandas is used for spreadsheet-like data structure and
scikit-bio is used for NGS oriented file formats. For user interaction in the
report, jQuery 2 and Vue.js are used in the same way as BioCloud website. For
interactive figures, highcharts.js plotting library is used.

% vim: set textwidth=79:
